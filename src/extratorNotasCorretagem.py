import pdfplumber
import pandas as pd
import re
import difflib
import os
import zipfile
import logging
import signal
import sys
import argparse
from io import BytesIO
from datetime import datetime
from typing import Dict, List, Optional
from tqdm import tqdm
from config import get_config

# Carregar configura√ß√µes
config = get_config()

# Configura√ß√£o de Logging
logging_level = config.get_logging_level()
logs_folder = config.resolve_path(config.get_logs_folder())

# Criar pasta de logs se n√£o existir
if not os.path.exists(logs_folder):
    os.makedirs(logs_folder)

# Gera nome do arquivo de log com timestamp
log_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
log_file = os.path.join(logs_folder, f"extracao_{log_timestamp}.log")

# Configurar logging com ambos console e arquivo
logging.basicConfig(
    level=getattr(logging, logging_level),
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%d/%m/%Y %H:%M:%S',
    handlers=[
        logging.FileHandler(log_file, encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# Flag para controle de interrup√ß√£o pelo usu√°rio (Ctrl+C)
stop_processing = False

def _handle_sigint(signum, frame):
    global stop_processing
    if not stop_processing:
        stop_processing = True
        logger.warning("‚è∏Ô∏è Interrup√ß√£o solicitada pelo usu√°rio (Ctrl+C). Finalizando ap√≥s o arquivo atual...")
    else:
        logger.error("‚úñ For√ßando sa√≠da imediata por novo Ctrl+C")
        sys.exit(1)

# Registra o handler para SIGINT
signal.signal(signal.SIGINT, _handle_sigint)

def _count_total_pdfs(caminho):
    """Conta o total estimado de PDFs que ser√£o processados a partir do caminho.
    Suporta arquivo ZIP √∫nico, pasta com PDFs e/ou ZIPs, e caminho para pasta com PDFs diretos.
    """
    try:
        if caminho.endswith('.zip') or (os.path.isfile(caminho) and zipfile.is_zipfile(caminho)):
            with zipfile.ZipFile(caminho, 'r') as z:
                return len([f for f in z.namelist() if f.endswith('.pdf')])

        if os.path.isdir(caminho):
            total = 0
            # PDFs diretos
            total += len([f for f in os.listdir(caminho) if f.endswith('.pdf')])
            # PDFs dentro de ZIPs na pasta
            zips = [f for f in os.listdir(caminho) if f.endswith('.zip')]
            for zf in zips:
                try:
                    with zipfile.ZipFile(os.path.join(caminho, zf), 'r') as z:
                        total += len([f for f in z.namelist() if f.endswith('.pdf')])
                except Exception:
                    continue
            return total

        return 0
    except Exception:
        return 0

# 1. Dicion√°rio De-Para para mapear nomes de ativos para Tickers
# Baseado nos exemplos dos fontes como "FIAGRO SUNO" [6] e "SUNO FIC FI" [7]
DE_PARA_TICKERS = {
    "PORTOSEGURO": "PSSA3",
    "PETROBRAS": "PETR3",
    "COPEL ON ED N1": "CPLE3",
    "NEOENERGIA ON NM": "NEOE3",
    "VALE ON": "VALE3",
    #"FIAGRO SUNO": "SNAG11",
    #"SUNO FIC FI": "SNCI11",
    #"FII ATRIO": "ARRI11",
    #"FII KINEA RI": "KNCR11",
    #"FII HSI CRI": "HSAF11"
}

def criar_bytesio_com_nome(conteudo, nome_arquivo):
    """Cria um BytesIO com atributo 'name' para compatibilidade."""
    bio = BytesIO(conteudo)
    bio.name = nome_arquivo
    return bio

def limpar_ticker(texto, dicionario):
    """Extrai o ticker ou usa o de-para para nomes conhecidos."""
    texto = texto.replace('\n', ' ').strip()
    # Tenta encontrar um padr√£o de Ticker (4 letras + 2 n√∫meros)
    match = re.search(r'[A-Z]{4}\d{2}', texto)
    if match:
        return match.group(0)
    
    # Se n√£o achar o Ticker puro, busca no dicion√°rio De-Para
    for nome, ticker in dicionario.items():
        if nome in texto.upper():
            return ticker
    return texto # Retorna o original se n√£o mapear


def _normalize_number(text):
    """Converte n√∫meros no formato brasileiro para ponto decimal (ex: '1.234,56' -> '1234.56')."""
    if not text:
        return ''
    s = str(text).strip()
    # Remove espa√ßos e s√≠mbolos extras
    s = s.replace('\xa0', '').replace(' ', '')
    # Se j√° tem apenas d√≠gitos e ponto, tenta retornar
    # Substitui milhares (.) e decimal (,) -> '.'
    # Primeiro, remove pontos que representam milhares
    # Mas cuidado: se houver apenas one dot and no comma, keep it
    if ',' in s:
        s = s.replace('.', '')
        s = s.replace(',', '.')
    else:
        # Remove any non-digit or dot
        s = re.sub(r'[^0-9\.]', '', s)
    return s


def _extract_operations_from_text(text, data_pregao, ticker_mapping):
    """Extrai opera√ß√µes de negocia√ß√£o diretamente do texto (fallback para tabelas faltantes).
    
    Padr√£o t√≠pico em notas de corretagem:
    1-BOVESPA C FRACIONARIO NOME DO ATIVO ON NM 1 40,00 40,00 D
    ou com prazo:
    1-BOVESPA C FRACIONARIO 01/00 NOME DO ATIVO ON NM 1 40,00 40,00 D
    
    Busca por: '1-BOVESPA' seguido de dados e retorna lista de opera√ß√µes encontradas.
    """
    operacoes = []
    
    if not text or '1-BOVESPA' not in text:
        return operacoes
    
    # Padr√£o: 1-BOVESPA seguido de opera√ß√£o C/V, tipo, NOME ATIVO (asset pode ter extra como DM 0,10), [opcional #], qtd, pre√ßo, pre√ßo, D/C
    # CORRIGIDO: Prazo DD/DD √© OPCIONAL agora - algumas notas n√£o t√™m prazo
    # CORRIGIDO: ([A-Z0-9\s]+?) para capturar nomes com n√∫meros (ex: ELETROBLAS, RAIADROGASIL ON NM)
    # CORRIGIDO: (.+?) para capturar asset name com extra info (ex: FORJA TAURUS DM 0,10), com #? para lidar com # nos dados
    pattern = r'1-BOVESPA\s+([CV])\s+(\w+)\s+(.+?)\s+#?\s*(\d+)\s+([\d.,]+)\s+([\d.,]+)\s+([DC])'
    
    for match in re.finditer(pattern, text, re.IGNORECASE):
        try:
            operacao = match.group(1).upper()
            ativo_nome = match.group(3).strip()
            
            # Os n√∫meros s√£o: quantidade, pre√ßo/ajuste, valor da opera√ß√£o
            qty_str = match.group(4)
            price_str = match.group(5)
            
            # Extrai ticker do nome do ativo
            ticker = _extract_ticker_from_cells([ativo_nome], ticker_mapping)
            if not ticker:
                continue
            
            quantidade = _normalize_number(qty_str)
            preco = _normalize_number(price_str)
            
            if not quantidade or not preco:
                continue
            
            operacoes.append({
                "Data": data_pregao,
                "Ticker": ticker,
                "Opera√ß√£o": operacao,
                "Quantidade": quantidade,
                "Pre√ßo": preco
            })
        except Exception:
            continue
    
    return operacoes


def _is_likely_header(cells):
    """Verifica se a linha parece ser um cabe√ßalho (nomes de colunas)."""
    if not cells:
        return False
    
    # Palavras t√≠picas de cabe√ßalhos
    header_keywords = ['data', 'ativo', 'especif', 'qtd', 'pre√ßo', 'valor', 'opera√ß√£o', 'nota', 'corretora']
    header_text = ' '.join([str(c).lower() for c in cells if c])
    
    # Se muitas c√©lulas t√™m pontos/dois-pontos, pode ser cabe√ßalho
    if header_text.count(':') > 3:
        return True
    
    # Se cont√©m m√∫ltiplas palavras de cabe√ßalho
    count = sum(1 for kw in header_keywords if kw in header_text)
    return count >= 2

def _is_valid_data_row(cells, is_negotiation_table=False):
    """Verifica se a linha parece ser um registro de negocia√ß√£o e n√£o cabe√ßalho/resumo/footer.
    
    Args:
        cells: List of cell values from the row
        is_negotiation_table: True if this is an 11-column negotiation table (less strict validation)
    """
    if not cells or len(cells) < 2:
        return False
    
    # Rejeita se parece ser cabe√ßalho
    if _is_likely_header(cells):
        return False
    
    # Palavras-chave que indicam linhas que N√ÉO s√£o negocia√ß√µes (headers, footers, summaries)
    skip_keywords = [
        'resumo', 'total', 'deb√™ntures', 'vendas', 'compras', 'op√ß√µes', 'termo',
        'taxa', 'emolumentos', 'transf', 'ativos', 'custodiante', 'clearing',
        'especifica√ß√µes', 'bovespa', 'cblc', 'cliente', 'c√≥digo', 'assessor',
        'participante', 'folha', 'data preg√£o', 'negocia√ß√£o', 'c.p.f', 'cnpj',
        'valor das oper', 'valor l√≠quido', 'qualificado', 'nota de negocia√ß√£o',
        'impostos', 'i.r.r.f', 'execu√ß√£o', 'cust√≥dia', 'bolsa', 'operacional',
        'custos', 'agente', 'qualificado', 'especifica√ß√µes diversas',
        'coluna q', 'liquida√ß√£o', 'agente do qualificado', '(*)', 'observa√ß√µes',
        'l√≠quido para',  # Settlement rows, not transactions
        'conta', 'saldo', 'c.m.c', 'participante destino'  # Account/routing info
    ]
    
    # Para tabelas de negocia√ß√£o (11 colunas), valida√ß√£o menos rigorosa
    if is_negotiation_table:
        # Apenas rejeita se tem palavras-chave muito √≥bvias (menos a de "bovespa")
        strict_skip = [
            'resumo', 'total', 'deb√™ntures', 'taxa', 'emolumentos', 'custos',
            'l√≠quido para', 'client', 'c√≥digo', 'c.p.f', 'cnpj', 'observa√ß√µes',
            '(*)', 'saldo', 'conta'
        ]
        row_text = ' '.join(cells).upper()
        for keyword in strict_skip:
            if keyword.upper() in row_text:
                return False
    else:
        # Para outras tabelas, valida√ß√£o rigorosa
        row_text = ' '.join(cells).upper()
        for keyword in skip_keywords:
            if keyword.upper() in row_text:
                return False
    
    # Uma linha v√°lida deve ter pelo menos um n√∫mero (quantidade ou pre√ßo)
    has_number = False
    for cell in cells:
        # Aceita tanto n√∫meros decimais quanto inteiros
        if re.search(r'\d', str(cell)):
            has_number = True
            break
    
    return has_number


def _normalize_text_for_comparison(text: str) -> str:
    """Normaliza texto para compara√ß√£o fuzzy de nomes de ativos.
    
    Remove espa√ßos extras, converte para mai√∫sculas, remove caracteres especiais.
    Exemplo: "SUZANO PAPEL ON NM" -> "SUZANO PAPEL ON NM" (sem mudan√ßa, mas padronizado)
    Exemplo: "SUZANOPAPEL ONNM" -> "SUZANO PAPEL ON NM" (ap√≥s tokeniza√ß√£o)
    """
    # Converte para mai√∫sculas
    text = text.upper().strip()
    
    # Remove m√∫ltiplos espa√ßos
    text = re.sub(r'\s+', ' ', text)
    
    # Remove h√≠fen (algumas varia√ß√µes podem usar h√≠fen)
    text = text.replace('-', ' ')
    
    # Remove caracteres especiais mantendo apenas letras, n√∫meros e espa√ßo
    text = re.sub(r'[^A-Z0-9\s]', '', text)
    
    # Remove espa√ßos novamente se criados pela remo√ß√£o de caracteres
    text = re.sub(r'\s+', ' ', text).strip()
    
    return text

def _extract_words_from_asset_name(text: str) -> set:
    """Extrai palavras significativas de um nome de ativo.
    
    Remove stopwords comuns e retorna um conjunto de palavras principais.
    Exemplo: "SUZANO PAPEL ON NM" -> {"SUZANO", "PAPEL", "ON", "NM"}
    """
    # Palavras comuns em tickers B3 que geralmente n√£o ajudam na identifica√ß√£o
    stopwords = {"ON", "PN", "NM", "N1", "N2", "ED", "EDUC", "PREFER", "ORDINARIA"}
    
    text = _normalize_text_for_comparison(text)
    words = set(text.split())
    
    # Remove stopwords, mas mant√©m pelo menos algo
    significant_words = words - stopwords
    if significant_words:
        return significant_words
    return words

def _fuzzy_match_asset_name(cell_text: str, mapping_name: str) -> bool:
    """Faz correspond√™ncia fuzzy entre nome de ativo da c√©lula e nome no mapeamento.
    
    Usa intersec√ß√£o de palavras principais. Se pelo menos 70% das palavras
    principais do mapeamento est√£o presentes na c√©lula, considera um match.
    
    Exemplos:
    - "SUZANO PAPEL ON NM" vs "SUZANO ON NM" -> True (78% match)
    - "SUZANO PAPEL ON NM" vs "SUZANOPAPEL ONNM" -> True (wordset match)
    - "SUZANO PAPEL ON NM" vs "PETROBRAS ON" -> False (0% overlap)
    """
    cell_words = _extract_words_from_asset_name(cell_text)
    mapping_words = _extract_words_from_asset_name(mapping_name)
    
    if not mapping_words:
        return False
    
    # Calcula intersec√ß√£o de palavras
    common_words = cell_words.intersection(mapping_words)
    match_percentage = len(common_words) / len(mapping_words)
    
    # Aceita match se pelo menos 70% das palavras mapeadas est√£o presentes
    # OU se h√° pelo menos 2 palavras em comum (para evitar falsos positivos)
    return match_percentage >= 0.70 or len(common_words) >= 2

def _fuzzy_match_score(cell_text: str, mapping_name: str) -> float:
    """Calcula score de correspond√™ncia fuzzy (0.0 a 1.0).
    
    Retorna o percentual de palavras do mapeamento presentes na c√©lula.
    Usado para ordenar m√∫ltiplos matches por especificidade.
    """
    cell_words = _extract_words_from_asset_name(cell_text)
    mapping_words = _extract_words_from_asset_name(mapping_name)
    
    if not mapping_words:
        return 0.0
    
    common_words = cell_words.intersection(mapping_words)
    # Prioriza matches com mais palavras em comum
    return len(common_words) / len(mapping_words)


def _string_similarity(a: str, b: str) -> float:
    """Retorna similaridade entre duas strings (0..1) usando SequenceMatcher."""
    if not a or not b:
        return 0.0
    a_norm = _normalize_text_for_comparison(a)
    b_norm = _normalize_text_for_comparison(b)
    # Use ratio direto
    return difflib.SequenceMatcher(None, a_norm, b_norm).ratio()

def _extract_ticker_from_cells(cells, ticker_mapping=None):
    """
    Extrai ticker da linha, buscando padr√£o B3 ou nome de ativo conhecido.
    
    Estrat√©gia:
    1. Procura por padr√£o ticker B3 (4 letras + 2 d√≠gitos)
    2. Busca em DE_PARA_TICKERS (hardcoded) com correspond√™ncia exata
    3. Busca em DE_PARA_TICKERS (hardcoded) com correspond√™ncia fuzzy (ordenada por score)
    4. Busca em ticker_mapping com correspond√™ncia exata
    5. Busca em ticker_mapping com correspond√™ncia fuzzy (ordenada por score, prioriza descri√ß√µes mais espec√≠ficas)
    """
    for cell in cells:
        cell_str = str(cell).strip()
        if not cell_str:
            continue
        
        # Passo 1: Tenta padr√£o ticker B3 (4 letras + 2 d√≠gitos)
        match = re.search(r'[A-Z]{4}\d{2}', cell_str)
        if match:
            return match.group(0)
        
        cell_str_normalized = _normalize_text_for_comparison(cell_str)
        
        # Passo 2: Tenta correspond√™ncia exata em DE_PARA (hardcoded)
        for nome, ticker in DE_PARA_TICKERS.items():
            if _normalize_text_for_comparison(nome) == cell_str_normalized:
                return ticker
        
        # Passo 3: Tenta correspond√™ncia fuzzy em DE_PARA (ordenada por score)
        best_match = None
        best_score = 0.0
        for nome, ticker in DE_PARA_TICKERS.items():
            if _fuzzy_match_asset_name(cell_str, nome):
                score = _fuzzy_match_score(cell_str, nome)
                if score > best_score:
                    best_score = score
                    best_match = ticker
        if best_match:
            return best_match
        
        # Passo 4: Tenta correspond√™ncia exata em ticker_mapping
        if ticker_mapping:
            for nome, ticker in ticker_mapping.items():
                if _normalize_text_for_comparison(nome) == cell_str_normalized:
                    return ticker
        
        # Passo 5: Tenta correspond√™ncia fuzzy em ticker_mapping (ordenada por score - mais espec√≠fica primeiro)
        if ticker_mapping:
            best_match = None
            best_score = 0.0
            for nome, ticker in ticker_mapping.items():
                if _fuzzy_match_asset_name(cell_str, nome):
                    score = _fuzzy_match_score(cell_str, nome)
                    # Prioriza matches com score mais alto (mais em comum)
                    # Em caso de empate, a ordem do dicion√°rio decide
                    if score > best_score:
                        best_score = score
                        best_match = ticker
            if best_match:
                return best_match
    
        # Passo 6: √öltima tentativa - correspond√™ncia por similaridade de string
        # (cobre erros de digita√ß√£o como 'BRASKEN' vs 'BRASKEM')
        try:
            for nome, ticker in DE_PARA_TICKERS.items():
                sim = _string_similarity(cell_str, nome)
                if sim >= 0.85:
                    return ticker
            if ticker_mapping:
                for nome, ticker in ticker_mapping.items():
                    sim = _string_similarity(cell_str, nome)
                    if sim >= 0.85:
                        return ticker
        except Exception:
            pass
    # Se n√£o encontrou padr√£o, retorna None (linha provavelmente n√£o √© v√°lida)
    return None

def _extract_year_from_filename(filename: str) -> Optional[int]:
    """Extrai o ano do nome do arquivo PDF.
    
    Procura por padr√µes como:
    - Clear 2026 01 Janeiro
    - Clear 2025 12 Dezembro
    - Arquivo_2024_01_Janeiro
    
    Args:
        filename: Nome do arquivo PDF
        
    Returns:
        Ano extra√≠do como inteiro, ou None se n√£o encontrar
    """
    # Padr√£o: 4 d√≠gitos que representam um ano (entre 1900 e 2100)
    match = re.search(r'\b(19|20)\d{2}\b', filename)
    if match:
        return int(match.group(0))
    return None

def _should_process_file(filename: str, target_year: Optional[int]) -> bool:
    """Verifica se o arquivo deve ser processado baseado no filtro de ano.
    
    Args:
        filename: Nome do arquivo
        target_year: Ano desejado (None = processar todos)
        
    Returns:
        True se deve processar, False caso contr√°rio
    """
    if target_year is None:
        return True
    
    file_year = _extract_year_from_filename(filename)
    if file_year is None:
        logger.warning(f"‚ö†Ô∏è  N√£o foi poss√≠vel extrair ano de: {filename}")
        return False
    
    return file_year == target_year


def processar_pdf(pdf_file, senha=None):
    dados_extraidos = []
    
    # Carrega mapeamento de tickers do arquivo de configura√ß√£o
    ticker_mapping = config.get_ticker_mapping()
    
    # Tratamento inteligente do nome do arquivo para diferentes tipos de entrada
    if isinstance(pdf_file, str):
        arquivo_nome = os.path.basename(pdf_file)
    else:
        # Para file objects e BytesIO, tenta obter 'name', sen√£o usa gen√©rico
        arquivo_nome = getattr(pdf_file, 'name', 'pdf_temporario.pdf')
    
    try:
        logger.info(f"üìÑ Processando arquivo: {arquivo_nome}")
        
        # Tenta abrir com senha se fornecida
        try:
            if senha:
                pdf = pdfplumber.open(pdf_file, password=senha)
            else:
                pdf = pdfplumber.open(pdf_file)
        except pdfplumber.utils.exceptions.PdfminerException:
            # Se falhar sem senha, tenta com a senha padr√£o da config
            try:
                senha_config = config.get_pdf_password()
                if senha_config:
                    pdf = pdfplumber.open(pdf_file, password=senha_config)
                else:
                    raise
            except:
                logger.warning(f"‚ö†Ô∏è  {arquivo_nome}: PDF protegido. Configure 'pdf.password' em application.properties")
                return dados_extraidos
        
        with pdf:
            total_paginas = len(pdf.pages)
            logger.debug(f"   Total de p√°ginas: {total_paginas}")
            
            for num_pagina, page in enumerate(pdf.pages, 1):
                try:
                    # Extra√ß√£o da Data (procura por "Data preg√£o") [3, 8, 9]
                    texto_topo = page.extract_text()
                    data_pregao = None
                    match_data = re.search(r'(\d{2}/\d{2}/\d{4})', texto_topo)
                    if match_data:
                        data_pregao = match_data.group(1)

                    # Extra√ß√£o da Tabela de Neg√≥cios [1, 2, 10]
                    tables = page.extract_tables()
                    registros_pagina = 0
                    
                    for table in tables:
                        if not table:
                            continue

                        # Detecta tabelas de negocia√ß√µes ‚Äî geralmente 11 colunas em muitas corretoras
                        num_cols = len(table[0]) if table and table[0] else 0
                        table_text = ' '.join(' '.join([str(c) for c in row if c]) for row in table)
                        is_negociacao = (num_cols == 11) or any(k in table_text for k in ['Data preg√£o', 'Nr. nota', 'Negocia√ß√£o', 'Especifica√ß√£o'])

                        if not is_negociacao:
                            # Heur√≠stica fallback: tente encontrar linhas que contenham quantidade+pre√ßo
                            # MAS USANDO A MESMA VALIDA√á√ÉO que acima
                            for row in table[1:]:
                                if not row or all(not (str(c).strip()) for c in row):
                                    continue

                                cells = [(c or '').strip() for c in row]
                                
                                # APLICAR VALIDA√á√ÉO: recusa headers/footers/summaries
                                if not _is_valid_data_row(cells, is_negotiation_table=False):
                                    continue
                                
                                try:
                                    # Verifica se √© uma linha v√°lida de negocia√ß√£o
                                    ticker = _extract_ticker_from_cells(cells, ticker_mapping)
                                    if not ticker:
                                        continue  # N√£o conseguiu extrair ticker v√°lido

                                    # Procura por padr√µes de n√∫mero (quantidade/price)
                                    possible_qty = None
                                    possible_price = None
                                    for c in cells:
                                        if re.search(r'\d+[\.,]\d+', c):
                                            # assume price-like
                                            if not possible_price:
                                                possible_price = _normalize_number(c)
                                        elif re.search(r'^\d+$', c.replace('.', '').replace(',', '')):
                                            if not possible_qty:
                                                possible_qty = _normalize_number(c)

                                    if not possible_qty and not possible_price:
                                        continue

                                    operacao = ''
                                    for c in cells:
                                        if ' C ' in f' {c} '.upper() or c.strip().upper() == 'C' or c.strip().upper() == 'V':
                                            operacao = 'C' if 'C' in c.upper() else 'V' if 'V' in c.upper() else ''
                                            if operacao:
                                                break

                                    dados_extraidos.append({
                                        "Data": data_pregao,
                                        "Ticker": ticker,
                                        "Opera√ß√£o": operacao,
                                        "Quantidade": possible_qty or '',
                                        "Pre√ßo": possible_price or ''
                                    })
                                    registros_pagina += 1
                                except Exception:
                                    continue
                            # fim heur√≠stica fallback
                        else:
                            # Para tabelas de negocia√ß√£o, processa TODAS as linhas (n√£o pula a primeira)
                            # A detec√ß√£o de cabe√ßalho √© feita dentro de _is_valid_data_row()
                            start_row = 0 if is_negociacao else 1
                            for row in table[start_row:]:
                                # Filtra linhas vazias
                                if not row or all(not (str(c).strip()) for c in row):
                                    continue

                                cells = [(c or '').strip() for c in row]
                                
                                # Verifica se √© uma linha v√°lida de negocia√ß√£o
                                if not _is_valid_data_row(cells, is_negotiation_table=True):
                                    continue

                                try:
                                    # Mapeamento comum observado em amostras:
                                    # col[2] = opera√ß√£o (C/V), col[5] = especifica√ß√£o (nome do ativo), col[7] = quantidade, col[8] = pre√ßo
                                    
                                    # Extrai ticker de forma robusta
                                    ticker = _extract_ticker_from_cells(cells, ticker_mapping)
                                    if not ticker:
                                        continue  # N√£o conseguiu extrair ticker v√°lido
                                    
                                    operacao = ''
                                    if len(cells) > 2 and cells[2]:
                                        operacao = 'C' if 'C' in cells[2].upper() else ('V' if 'V' in cells[2].upper() else '')

                                    quantidade_raw = cells[7] if len(cells) > 7 else ''
                                    preco_raw = cells[8] if len(cells) > 8 else ''

                                    quantidade = _normalize_number(quantidade_raw)
                                    preco = _normalize_number(preco_raw)

                                    # Se n√£o houver quantidade nem pre√ßo, provavelmente n√£o √© linha de neg√≥cio
                                    if not quantidade and not preco:
                                        continue

                                    dados_extraidos.append({
                                        "Data": data_pregao,
                                        "Ticker": ticker,
                                        "Opera√ß√£o": operacao,
                                        "Quantidade": quantidade,
                                        "Pre√ßo": preco
                                    })
                                    registros_pagina += 1
                                except Exception as e:
                                    logger.debug(f"   ‚ö†Ô∏è  Erro ao extrair linha: {str(e)}")
                                    continue
                    
                    # FALLBACK: Extrair opera√ß√µes diretamente do texto como backup
                    # Isso trata casos onde pdfplumber falha ao extrair todas as linhas das tabelas
                    # (ex: opera√ß√µes do meio ficam faltando na divis√£o de tabelas)
                    if data_pregao:
                        operacoes_texto = _extract_operations_from_text(texto_topo, data_pregao, ticker_mapping)
                        
                        # Criar assinatra de opera√ß√µes j√° extra√≠das (Data + Ticker + Quantidade + Pre√ßo)
                        # para evitar duplicatas
                        operacoes_existentes = set()
                        for op in dados_extraidos:
                            sig = (op.get('Data'), op.get('Ticker'), op.get('Quantidade'), op.get('Pre√ßo'))
                            operacoes_existentes.add(sig)
                        
                        # Adicionar apenas opera√ß√µes novas do texto
                        novas_operacoes = 0
                        for op in operacoes_texto:
                            sig = (op.get('Data'), op.get('Ticker'), op.get('Quantidade'), op.get('Pre√ßo'))
                            if sig not in operacoes_existentes:
                                dados_extraidos.append(op)
                                novas_operacoes += 1
                        
                        if novas_operacoes > 0:
                            logger.debug(f"   ‚ÑπÔ∏è  Adicionadas {novas_operacoes} opera√ß√£o(√µes) extra√≠da(s) do texto")
                            registros_pagina += novas_operacoes
                    
                    if registros_pagina > 0:
                        logger.debug(f"   ‚úì P√°gina {num_pagina}/{total_paginas}: {registros_pagina} registro(s) extra√≠do(s)")
                        
                except Exception as e:
                    logger.error(f"   ‚úó Erro ao processar p√°gina {num_pagina}: {str(e)}")
                    continue
        
        total_registros = len(dados_extraidos)
        if total_registros > 0:
            logger.info(f"‚úì {arquivo_nome}: {total_registros} registro(s) extra√≠do(s) com sucesso")
        else:
            logger.warning(f"‚ö†Ô∏è  {arquivo_nome}: Nenhum registro extra√≠do")
            
    except FileNotFoundError:
        logger.error(f"‚úó Arquivo n√£o encontrado: {arquivo_nome}")
    except Exception as e:
        logger.error(f"‚úó Erro ao processar {arquivo_nome}: {str(e)}")
    
    return dados_extraidos

def analisar_pasta_ou_zip(caminho, year_filter: Optional[int] = None):
    todos_dados = []
    arquivos_processados = 0
    arquivos_erro = 0
    arquivos_ignorados = 0

    try:
        logger.info("=" * 60)
        logger.info("üöÄ INICIANDO PROCESSAMENTO")
        if year_filter is not None:
            logger.info(f"üîç Filtro de ano ativo: {year_filter}")
        logger.info("=" * 60)

        # Resolve caminho relativo se necess√°rio
        if not os.path.isabs(caminho):
            caminho_resolvido = os.path.join(os.path.dirname(__file__), caminho)
            if os.path.exists(caminho_resolvido):
                caminho = caminho_resolvido

        # Valida√ß√£o do caminho fornecido
        if not os.path.exists(caminho):
            logger.error(f"‚úó Caminho n√£o encontrado: {caminho}")
            return pd.DataFrame()

        total_arquivos = _count_total_pdfs(caminho)
        logger.info(f"üì• Total estimado de PDFs para processar: {total_arquivos}")

        if total_arquivos == 0:
            logger.warning("‚ö†Ô∏è  Nenhum arquivo PDF encontrado para processar")
            return pd.DataFrame()

        # Cria a lista de tarefas (uniformiza arquivos diretos e dentro de ZIPs)
        tarefas = []

        if caminho.endswith('.zip') or (os.path.isfile(caminho) and zipfile.is_zipfile(caminho)):
            tarefas_tipo = 'single_zip'
            with zipfile.ZipFile(caminho, 'r') as z:
                for f in z.namelist():
                    if f.endswith('.pdf'):
                        # Aplica filtro de ano se especificado
                        if _should_process_file(f, year_filter):
                            tarefas.append({'type': 'zip_entry', 'zip': caminho, 'name': f})
                        else:
                            arquivos_ignorados += 1

        elif os.path.isdir(caminho):
            # PDFs diretos
            for f in os.listdir(caminho):
                if f.endswith('.pdf'):
                    # Aplica filtro de ano se especificado
                    if _should_process_file(f, year_filter):
                        tarefas.append({'type': 'file', 'path': os.path.join(caminho, f)})
                    else:
                        arquivos_ignorados += 1

            # PDFs dentro de ZIPs na pasta
            for zf in [f for f in os.listdir(caminho) if f.endswith('.zip')]:
                caminho_zip = os.path.join(caminho, zf)
                try:
                    with zipfile.ZipFile(caminho_zip, 'r') as z:
                        for entry in z.namelist():
                            if entry.endswith('.pdf'):
                                # Aplica filtro de ano se especificado
                                if _should_process_file(entry, year_filter):
                                    tarefas.append({'type': 'zip_entry', 'zip': caminho_zip, 'name': entry})
                                else:
                                    arquivos_ignorados += 1
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è  N√£o foi poss√≠vel listar ZIP {zf}: {str(e)}")

        else:
            logger.error(f"‚úó Caminho n√£o √© arquivo ZIP ou pasta: {caminho}")
            return pd.DataFrame()

        # Barra de progresso global para todos os PDFs
        with tqdm(total=len(tarefas), desc="üì• Processando PDFs", unit="arquivo") as pbar:
            try:
                for tarefa in tarefas:
                    if stop_processing:
                        logger.warning("‚è∏Ô∏è Interrup√ß√£o detectada ‚Äî finalizando processamento ap√≥s o arquivo atual.")
                        break

                    if tarefa['type'] == 'file':
                        try:
                            dados = processar_pdf(tarefa['path'])
                            todos_dados.extend(dados)
                            arquivos_processados += 1
                        except Exception as e:
                            logger.error(f"‚úó Erro ao processar {tarefa['path']}: {str(e)}")
                            arquivos_erro += 1
                        finally:
                            pbar.update(1)

                    elif tarefa['type'] == 'zip_entry':
                        try:
                            with zipfile.ZipFile(tarefa['zip'], 'r') as z:
                                with z.open(tarefa['name']) as f:
                                    bio = criar_bytesio_com_nome(f.read(), os.path.basename(tarefa['name']))
                                    dados = processar_pdf(bio)
                                    todos_dados.extend(dados)
                                    arquivos_processados += 1
                        except Exception as e:
                            logger.error(f"‚úó Erro ao processar {tarefa['name']} do ZIP {os.path.basename(tarefa['zip'])}: {str(e)}")
                            arquivos_erro += 1
                        finally:
                            pbar.update(1)

            except KeyboardInterrupt:
                logger.warning("‚ö†Ô∏è  Execu√ß√£o interrompida pelo usu√°rio (KeyboardInterrupt). Salvando progresso parcial...")
                # stop_processing j√° ser√° True pelo handler; fora do la√ßo iremos exportar o parcial

        # Resumo final
        logger.info("\n" + "=" * 60)
        logger.info("üìä RESUMO DO PROCESSAMENTO")
        logger.info("=" * 60)
        logger.info(f"‚úì Arquivos processados com sucesso: {arquivos_processados}")
        if arquivos_erro > 0:
            logger.warning(f"‚ö†Ô∏è  Arquivos com erro: {arquivos_erro}")
        if arquivos_ignorados > 0:
            logger.info(f"‚è≠Ô∏è Arquivos ignorados (fora do filtro de ano): {arquivos_ignorados}")
        logger.info(f"üìà Total de registros extra√≠dos: {len(todos_dados)}")
        logger.info("=" * 60)

        df = pd.DataFrame(todos_dados)
        return df

    except Exception as e:
        logger.error(f"‚úó Erro inesperado durante o processamento: {str(e)}")
        logger.info("=" * 60)
        return pd.DataFrame()


def ordenar_dados_por_data(df):
    """Ordena o DataFrame por Data (do mais antigo para o mais recente).
    
    Args:
        df (pd.DataFrame): DataFrame com coluna 'Data' em formato DD/MM/YYYY
        
    Returns:
        pd.DataFrame: DataFrame ordenado por data
    """
    if df.empty:
        return df
    
    try:
        # Converte coluna Data para datetime (formato DD/MM/YYYY)
        df['Data'] = pd.to_datetime(df['Data'], format='%d/%m/%Y')
        
        # Ordena por Data (do mais antigo para o mais recente)
        df = df.sort_values('Data', ascending=True)
        
        # Converte Data de volta para string no formato original DD/MM/YYYY
        df['Data'] = df['Data'].dt.strftime('%d/%m/%Y')
        
        logger.info("‚úì Dados ordenados por data (mais antigo para o mais recente)")
        return df.reset_index(drop=True)
    
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è  Erro ao ordenar dados por data: {str(e)}")
        return df


def criar_aba_arvore(df):
    """Cria um DataFrame com estrutura de √°rvore partindo a Data em Ano/M√™s/Dia.
    
    As colunas de Ano, M√™s e Dia s√£o preenchidas apenas ao mudar o per√≠odo,
    criando uma visualiza√ß√£o em √°rvore hier√°rquica.
    
    Args:
        df (pd.DataFrame): DataFrame ordenado com coluna 'Data' em DD/MM/YYYY
        
    Returns:
        pd.DataFrame: DataFrame com colunas [Ano, M√™s, Dia, Data, Ticker, Opera√ß√£o, 
                      Quantidade, Pre√ßo]
    """
    if df.empty:
        return pd.DataFrame()
    
    try:
        # Cria c√≥pia para n√£o modificar o original
        df_arvore = df.copy()
        
        # Extrai Ano, M√™s, Dia da Data
        df_arvore['Data_dt'] = pd.to_datetime(df_arvore['Data'], format='%d/%m/%Y')
        df_arvore['Ano'] = df_arvore['Data_dt'].dt.year.astype(str)
        df_arvore['Mes'] = df_arvore['Data_dt'].dt.month.astype(str).str.zfill(2)
        df_arvore['Dia'] = df_arvore['Data_dt'].dt.day.astype(str).str.zfill(2)
        
        # Monta chave de per√≠odo (Ano-M√™s-Dia)
        df_arvore['periodo'] = df_arvore['Ano'] + '-' + df_arvore['Mes'] + '-' + df_arvore['Dia']
        
        # Identifica onde o per√≠odo muda em rela√ß√£o √† linha anterior
        df_arvore['periodo_anterior'] = df_arvore['periodo'].shift(1)
        df_arvore['muda_ano'] = df_arvore['Ano'] != df_arvore['Ano'].shift(1)
        df_arvore['muda_mes'] = (df_arvore['Ano'] + '-' + df_arvore['Mes']) != \
                                  (df_arvore['Ano'].shift(1) + '-' + df_arvore['Mes'].shift(1))
        df_arvore['muda_dia'] = df_arvore['periodo'] != df_arvore['periodo'].shift(1)
        
        # Preenche Ano, M√™s, Dia apenas quando mudam (criando efeito de √°rvore)
        df_arvore.loc[~df_arvore['muda_ano'], 'Ano'] = ''
        df_arvore.loc[~df_arvore['muda_mes'], 'Mes'] = ''
        df_arvore.loc[~df_arvore['muda_dia'], 'Dia'] = ''
        
        # Ordena colunas: Ano, M√™s, Dia, Data, Ticker, Opera√ß√£o, Quantidade, Pre√ßo
        colunas_arvore = ['Ano', 'Mes', 'Dia', 'Data', 'Ticker', 'Opera√ß√£o', 'Quantidade', 'Pre√ßo']
        df_arvore = df_arvore[colunas_arvore]
        
        # Remove coluna auxiliar de data convertida
        if 'Data_dt' in df_arvore.columns:
            df_arvore = df_arvore.drop('Data_dt', axis=1, errors='ignore')
        
        return df_arvore.reset_index(drop=True)
    
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è  Erro ao criar aba de √°rvore: {str(e)}")
        return df.copy()


def exportar_dados(df, formato=None):
    """Exporta os dados extra√≠dos para o formato especificado.
    
    Args:
        df (pd.DataFrame): DataFrame com os dados extra√≠dos
        formato (str): Formato de sa√≠da (csv, xlsx, json). Se None, usa config.
        
    Returns:
        bool: True se exportado com sucesso, False caso contr√°rio
    """
    if df.empty:
        logger.warning("‚ö†Ô∏è  Nenhum dado para exportar")
        return False
    
    try:
        # Ordena dados por data antes de exportar
        df = ordenar_dados_por_data(df)
        # Obt√©m formato da config se n√£o fornecido
        if formato is None:
            formato = config.get_output_format().lower()
        
        # Cria pasta de output
        pasta_output = config.get_output_folder()
        if not os.path.exists(pasta_output):
            os.makedirs(pasta_output)
            logger.info(f"‚úì Pasta de sa√≠da criada: {pasta_output}")
        
        # Gera nome do arquivo com timestamp
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # Exporta no formato escolhido
        if formato == 'csv':
            arquivo_saida = os.path.join(pasta_output, f"dados_extraidos_{timestamp}.csv")
            df.to_csv(arquivo_saida, index=False, encoding='utf-8-sig')
            logger.info(f"‚úì Dados exportados para CSV: {arquivo_saida}")
            logger.info(f"   Linhas: {len(df)} | Colunas: {len(df.columns)}")
            
        elif formato == 'xlsx':
            arquivo_saida = os.path.join(pasta_output, f"dados_extraidos_{timestamp}.xlsx")
            # Cria writer para m√∫ltiplas abas
            with pd.ExcelWriter(arquivo_saida, engine='openpyxl') as writer:
                # Aba 1: Dados completos e ordenados
                df.to_excel(writer, sheet_name="Dados", index=False)
                logger.info(f"‚úì Aba 'Dados' criada: {len(df)} linhas, {len(df.columns)} colunas")
                
                # Aba 2: Estrutura de √°rvore (Ano/M√™s/Dia hier√°rquicos)
                df_arvore = criar_aba_arvore(df)
                df_arvore.to_excel(writer, sheet_name="√Årvore", index=False)
                logger.info(f"‚úì Aba '√Årvore' criada: {len(df_arvore)} linhas (estrutura hier√°rquica)")
            
            logger.info(f"‚úì Arquivo XLSX exportado com 2 abas: {arquivo_saida}")
            
        elif formato == 'json':
            arquivo_saida = os.path.join(pasta_output, f"dados_extraidos_{timestamp}.json")
            df.to_json(arquivo_saida, orient='records', indent=2, force_ascii=False)
            logger.info(f"‚úì Dados exportados para JSON: {arquivo_saida}")
            logger.info(f"   Linhas: {len(df)} | Colunas: {len(df.columns)}")
        else:
            logger.error(f"‚úó Formato n√£o suportado: {formato}")
            logger.info("   Formatos suportados: csv, xlsx, json")
            return False
        
        return True
        
    except ImportError as e:
        if 'openpyxl' in str(e) and formato == 'xlsx':
            logger.error("‚úó Erro: openpyxl n√£o instalado. Para usar XLSX, instale com:")
            logger.error("   pip install openpyxl")
        else:
            logger.error(f"‚úó Biblioteca n√£o encontrada: {str(e)}")
        return False
    except Exception as e:
        logger.error(f"‚úó Erro ao exportar dados: {str(e)}")
        return False


if __name__ == "__main__":
    # Cria parser de argumentos de linha de comando
    parser = argparse.ArgumentParser(
        description="Extrator de Notas de Negocia√ß√£o da Clear Corretora",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Exemplos de uso:
  python3 extratorNotasCorretagem.py                    # Processa todos os PDFs
  python3 extratorNotasCorretagem.py --year 2024        # Processa apenas PDFs de 2024
  python3 extratorNotasCorretagem.py -y 2026            # Processa apenas PDFs de 2026
        """
    )
    parser.add_argument(
        '--year', '-y',
        type=int,
        default=None,
        help='Filtrar por ano (extrair apenas PDFs com esse ano no nome do arquivo)'
    )
    
    args = parser.parse_args()
    year_filter = args.year
    
    # Caminho da pasta com os arquivos de entrada
    caminho_pasta = config.get_input_folder()
    
    # Resolve o caminho absoluto
    caminho_absoluto = config.resolve_path(caminho_pasta)
    
    logger.info(f"üìÇ Diret√≥rio de entrada: {caminho_pasta}")
    logger.info(f"üîç Caminho absoluto: {caminho_absoluto}\n")
    
    # Se a pasta n√£o existe, tenta informar melhor
    if not os.path.exists(caminho_absoluto):
        logger.error(f"‚úó Pasta n√£o encontrada: {caminho_absoluto}")
        logger.info("\nüí° Dicas:")
        logger.info("   1. Verifique se o caminho est√° correto no arquivo application.properties")
        logger.info("   2. Certifique-se de que a pasta especificada existe")
        logger.info("   3. Coloque seus arquivos PDF ou ZIP dentro dessa pasta")
    else:
        logger.info("‚úì Pasta encontrada. Processando...\n")
        df = analisar_pasta_ou_zip(caminho_absoluto, year_filter=year_filter)
        
        if not df.empty:
            logger.info(f"\nüìã Primeiras linhas dos dados extra√≠dos:")
            logger.info(f"\n{df.head()}")
            
            # Exporta os dados
            logger.info("\n" + "=" * 60)
            logger.info("üíæ EXPORTANDO DADOS")
            logger.info("=" * 60)
            formato = config.get_output_format()
            sucesso = exportar_dados(df, formato)
            
            if sucesso:
                logger.info(f"\n‚úì Processamento conclu√≠do com sucesso!")
            else:
                logger.warning("‚ö†Ô∏è  Dados extra√≠dos mas n√£o foi poss√≠vel exportar.")
        else:
            logger.warning("‚ö†Ô∏è  Nenhum dado foi extra√≠do. Verifique os arquivos PDF na pasta.")
